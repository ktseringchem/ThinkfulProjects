{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Review of the movie reviews\n",
    "\n",
    "### by Karma Tsering\n",
    "My goal is to predict sentiment of each movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation of this project\n",
    "There are too many movies and too many people with opnion on them. Its doubtful that any one would want to read these reviews, certly not companies that need to \n",
    "\n",
    "### Data Source\n",
    "This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. \n",
    "\n",
    "The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg). \n",
    "\n",
    "In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saves list of files names to loop though\n",
    "neg_train_file_names = [f for f in listdir('aclImdb\\\\train\\\\neg') if isfile(join('aclImdb\\\\train\\\\neg', f))]\n",
    "pos_train_file_names = [f for f in listdir('aclImdb\\\\train\\\\pos') if isfile(join('aclImdb\\\\train\\\\pos', f))]\n",
    "neg_test_file_names = [f for f in listdir('aclImdb\\\\test\\\\neg') if isfile(join('aclImdb\\\\test\\\\neg', f))]\n",
    "pos_test_file_names = [f for f in listdir('aclImdb\\\\test\\\\pos') if isfile(join('aclImdb\\\\test\\\\pos', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The reviews are saved in each individual files, so I looped through each file and saved the content of the files as \n",
    "#and assigned a sentement based on which folder the file was in (neg = 0/ pos = 1).\n",
    "review_df1 = []\n",
    "review_df2 = []\n",
    "review_df3 = []\n",
    "review_df4 = []\n",
    "\n",
    "for file in neg_train_file_names:\n",
    "    file1_open = open(\"aclImdb\\\\train\\\\neg\\\\{}\".format(file), encoding=\"utf8\")\n",
    "    file1_content = file1_open.read()\n",
    "    review_df1.append([file1_content, 0])\n",
    "    \n",
    "for file in pos_train_file_names:\n",
    "    file1_open = open(\"aclImdb\\\\train\\\\pos\\\\{}\".format(file), encoding=\"utf8\")\n",
    "    file1_content = file1_open.read()\n",
    "    review_df2.append([file1_content, 1])\n",
    "\n",
    "for file in neg_test_file_names:\n",
    "    file1_open = open(\"aclImdb\\\\test\\\\neg\\\\{}\".format(file), encoding=\"utf8\")\n",
    "    file1_content = file1_open.read()\n",
    "    review_df3.append([file1_content, 0])\n",
    "\n",
    "for file in pos_test_file_names:\n",
    "    file1_open = open(\"aclImdb\\\\test\\\\pos\\\\{}\".format(file), encoding=\"utf8\")\n",
    "    file1_content = file1_open.read()\n",
    "    review_df4.append([file1_content, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put them in DataFrame format\n",
    "review_df1 = pd.DataFrame(review_df1)\n",
    "review_df2 = pd.DataFrame(review_df2)\n",
    "review_df3 = pd.DataFrame(review_df3)\n",
    "review_df4 = pd.DataFrame(review_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I must be honest, I like romantic comedies, bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw most of the episodes of RMFTM as a teena...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chuck Norris stars as Danny, a cop who took do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Tycus' is almost as bad as a science fiction ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a very realistic movie. It's the most ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I absolutely love all of Tom Robbins books, so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I saw this film at a store in the cheap sectio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'll start with what I liked.&lt;br /&gt;&lt;br /&gt;I rea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This is an admirable attempt from first time f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is your typical cheerful and colorful MGM...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  I must be honest, I like romantic comedies, bu...       0\n",
       "1  I saw most of the episodes of RMFTM as a teena...       0\n",
       "2  Chuck Norris stars as Danny, a cop who took do...       0\n",
       "3  'Tycus' is almost as bad as a science fiction ...       0\n",
       "4  This is a very realistic movie. It's the most ...       1\n",
       "5  I absolutely love all of Tom Robbins books, so...       0\n",
       "6  I saw this film at a store in the cheap sectio...       1\n",
       "7  I'll start with what I liked.<br /><br />I rea...       0\n",
       "8  This is an admirable attempt from first time f...       0\n",
       "9  This is your typical cheerful and colorful MGM...       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put all the above dataframe in one dataframe\n",
    "review_df = pd.concat([review_df1, review_df2, review_df3, review_df4]).sample(frac=1).reset_index(drop=True)\n",
    "review_df.columns = ['Review', 'Rating']\n",
    "print(review_df.shape)\n",
    "review_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame created above\n",
    "\n",
    "# text file for making BOW\n",
    "Put all the reviews in one string in perperation for extrating key words to be used as feature later on for creating vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick 2000 words and make BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I cleanded the text as much as possible so to reduce load for nlp processing and to increase the number of vocas for feature\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "allwords = \" \"\n",
    "list_of_review = []\n",
    "Review = []\n",
    "pattern = \"[-*:!&$%',.\\\\?\\\"/<>()\\d]\"\n",
    "pattern2 = r\"\\bbr\\b\"\n",
    "pattern3 = r'\\bA\\b'\n",
    "pattern4 = r'--'\n",
    "#pattern5 = '.+;$'\n",
    "\n",
    "for review in review_df['Review']:\n",
    "    #cleaning up each review text by removing above patters\n",
    "    mov_review = re.sub(pattern, \"\", review)\n",
    "    mov_review = re.sub(pattern2, \"\", mov_review)\n",
    "    mov_review = re.sub(pattern3, \"\", mov_review)\n",
    "    mov_review = re.sub(pattern4, \"\", mov_review)\n",
    "    #mov_review = re.sub(pattern5, \"\", mov_review)\n",
    "    mov_review = mov_review.split()\n",
    "    mov_review = [x for x in mov_review\n",
    "                if not x == ' '\n",
    "                and not x == 's'\n",
    "                and not x == 'I'\n",
    "                and not x == 'movie'\n",
    "                and not x == '-PRON-'\n",
    "                and not x == 'film'\n",
    "                and not x == '\\x96'\n",
    "                and not x == '!'\n",
    "                and x not in stopWords]\n",
    "\n",
    "    #Collect all review as string to make BOW later\n",
    "    #allwords = allwords + mov_review\n",
    "    Onefull_review = ''\n",
    "    for review_words in mov_review:\n",
    "        Onefull_review = Onefull_review + ' ' + review_words\n",
    "    list_of_review.append(Onefull_review)\n",
    "    \n",
    "    allwords = allwords + ' ' +' '.join(mov_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above the string text has been worked to reduce the nlp processing work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nlp processing\n",
    "#there seems to a limit of million char for nlp processing so I did it twice to get more feature\n",
    "allwords_doc1 = nlp(allwords[0:999999])\n",
    "allwords_doc2 = nlp(allwords[999999:1999998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing stop words help in creating meaningful features\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#save only the words we care about\n",
    "def bag_of_words(text):\n",
    "    allwords = [token.lemma_\n",
    "                    for token in text\n",
    "                    if not token.is_punct\n",
    "                    and not token.is_stop]\n",
    "    allwords = [x for x in allwords\n",
    "                if not x == ' '\n",
    "                and not x == 's'\n",
    "                and not x == 'movie'\n",
    "                and not x == '-PRON-'\n",
    "                and not x == 'film'\n",
    "                and x not in stopWords]\n",
    "   \n",
    "    return allwords\n",
    "\n",
    "#run both processed nlp text\n",
    "word_count1 = [item[0] for item  in Counter(bag_of_words(allwords_doc1)).most_common(4000)]\n",
    "word_count2 = [item[0] for item  in Counter(bag_of_words(allwords_doc2)).most_common(4000)]\n",
    "\n",
    "#combaining both bag of words and removing duplicates\n",
    "word_count = set(word_count1 + word_count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a df with word in the bag of words, make feature of them, and count how many there are in each review. Those count will be the data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_features(review_df, word_count):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=word_count)\n",
    "    df['text_sentence'] = review_df['Review'][0:20000]\n",
    "    df['text_source'] = review_df['Rating'][0:20000]\n",
    "    df.loc[:, word_count] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        sentence = nlp(sentence)\n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in word_count\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this takes too long so I only included 20,000 review. \n",
    "#word_counts = bow_features(review_df, word_count)\n",
    "#word_counts\n",
    "#I ran the above code and saved the file as 'Review_with_feature.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>relevant</th>\n",
       "      <th>compare</th>\n",
       "      <th>uplift</th>\n",
       "      <th>campus</th>\n",
       "      <th>shrink</th>\n",
       "      <th>metal</th>\n",
       "      <th>farscape</th>\n",
       "      <th>earlier</th>\n",
       "      <th>ultimate</th>\n",
       "      <th>...</th>\n",
       "      <th>loud</th>\n",
       "      <th>flavor</th>\n",
       "      <th>arc</th>\n",
       "      <th>assassination</th>\n",
       "      <th>commentator</th>\n",
       "      <th>contract</th>\n",
       "      <th>charming</th>\n",
       "      <th>underground</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>WWII veterans return home and find it hard to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is an incredible movie that begins slowly...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>It worked! Director Christian Duguay created a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spin-offs, for somebody who don't know, are no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...about this film was the title song. After 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  relevant  compare  uplift  campus  shrink  metal  farscape  \\\n",
       "0           0         0        0       0       0       0      0         0   \n",
       "1           1         0        0       0       0       0      0         0   \n",
       "2           2         0        0       0       0       0      0         0   \n",
       "3           3         0        0       0       0       0      0         0   \n",
       "4           4         0        0       0       0       0      0         0   \n",
       "\n",
       "   earlier  ultimate     ...       loud  flavor  arc  assassination  \\\n",
       "0        0         0     ...          0       0    0              0   \n",
       "1        0         0     ...          0       0    0              0   \n",
       "2        0         0     ...          0       0    0              0   \n",
       "3        0         0     ...          0       0    0              0   \n",
       "4        0         0     ...          0       0    0              0   \n",
       "\n",
       "   commentator  contract  charming  underground  \\\n",
       "0            0         0         0            0   \n",
       "1            0         0         0            0   \n",
       "2            0         0         0            0   \n",
       "3            0         0         0            0   \n",
       "4            0         0         0            0   \n",
       "\n",
       "                                       text_sentence  text_source  \n",
       "0  WWII veterans return home and find it hard to ...            1  \n",
       "1  This is an incredible movie that begins slowly...            1  \n",
       "2  It worked! Director Christian Duguay created a...            1  \n",
       "3  Spin-offs, for somebody who don't know, are no...            1  \n",
       "4  ...about this film was the title song. After 3...            0  \n",
       "\n",
       "[5 rows x 5052 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading df from saved file\n",
    "review_feature_df = pd.read_csv('Review_with_feature.csv', encoding=\"ISO-8859-1\")\n",
    "review_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trying gridsearch on subsection of the dataset becasue it is process intensive but it didn't help so result won't be included\n",
    "review_feature_df2 = review_feature_df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the dataset into train and test sets, with test size of 40% becasue it gives me better result\n",
    "X = review_feature_df.drop(['text_sentence', 'text_source'], 1)\n",
    "y = review_feature_df['text_source']\n",
    "\n",
    "X_pca = PCA(n_components=5).fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, \n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating clusters\n",
    "### K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "text_source     0     1\n",
      "row_0                  \n",
      "0            3846  4301\n",
      "1            2153  1700\n",
      "Comparing k-means clusters against the data:\n",
      "text_source     0     1\n",
      "row_0                  \n",
      "0            1052  1005\n",
      "1            4947  4996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Normalize the data.\n",
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm = normalize(X_test)\n",
    "\n",
    "# Reduce it to two components.\n",
    "X_train_pca = PCA(n_components=0.95).fit_transform(X_train_norm)\n",
    "X_test_pca = PCA(n_components=0.95).fit_transform(X_test_norm)\n",
    "\n",
    "KMean1 = KMeans(n_clusters=2, random_state=42)\n",
    "KMean2 = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "KMean1.fit(X_train_pca)\n",
    "KMean2.fit(X_train)\n",
    "\n",
    "y_pred1 = KMean1.predict(X_train_pca)\n",
    "y_pred2 = KMean2.predict(X_train)\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred1, y_train))\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred2, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying pca does not seem to improve the result therefore I left it out. These result are not that good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "Comparing k-means clusters against the data:\n",
      "text_source     0     1\n",
      "row_0                  \n",
      "0             656   699\n",
      "1            3345  3300\n"
     ]
    }
   ],
   "source": [
    "#y_pred1 = KMean1.predict(X_test_pca)\n",
    "y_pred2 = KMean2.predict(X_test)\n",
    "\n",
    "print('Comparing k-means clusters against the data:')\n",
    "#print(pd.crosstab(y_pred1, y_test))\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a similer result for test set. The model can't predict well but it is overfitting either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8171666666666667\n",
      "\n",
      "Test set score: 0.78925\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result seems the best and does not seem to have overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 5049) (12000,)\n",
      "Training set score: 0.98375\n",
      "\n",
      "Test set score: 0.84275\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5887</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>5918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0           0     1\n",
       "text_source            \n",
       "0            5887   112\n",
       "1              83  5918"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First trying some classicication models starting with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "y_pred = lr.predict(X_train)\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3383</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640</td>\n",
       "      <td>3359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0           0     1\n",
       "text_source            \n",
       "0            3383   618\n",
       "1             640  3359"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "pd.crosstab(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has higher predictivity but suffers form large overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9933333333333333\n",
      "\n",
      "Test set score: 0.75725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5991</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>5929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0           0     1\n",
       "text_source            \n",
       "0            5991     8\n",
       "1              72  5929"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "y_pred = rfc.predict(X_train)\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3300</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1241</td>\n",
       "      <td>2758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0           0     1\n",
       "text_source            \n",
       "0            3300   701\n",
       "1            1241  2758"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfc.predict(X_test)\n",
    "pd.crosstab(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too large a overfitting to be any good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8171666666666667\n",
      "\n",
      "Test set score: 0.78925\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives me very smiler result as minibatchkmeans model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.66725\n",
      "col_0           0     1\n",
      "text_source            \n",
      "0            3861  2138\n",
      "1            1855  4146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', svm.score(X_train, y_train))\n",
    "y_pred1 = svm.predict(X_train)\n",
    "print(pd.crosstab(y_train, y_pred1))\n",
    "\n",
    "#print('\\nTest set score:', svm.score(X_test, y_test))\n",
    "#y_pred2 = svm.predict(X_test)\n",
    "#pd.crosstab(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.64      0.66      5999\n",
      "          1       0.66      0.69      0.67      6001\n",
      "\n",
      "avg / total       0.67      0.67      0.67     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics iport classification_report, confusion_matrix\n",
    "print(classification_report(y_train, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6660833333333334\n",
      "col_0           0     1\n",
      "text_source            \n",
      "0            3798  2201\n",
      "1            1806  4195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=10, gamma=1, kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', svm.score(X_train, y_train))\n",
    "y_pred1 = svm.predict(X_train)\n",
    "print(pd.crosstab(y_train, y_pred1))\n",
    "\n",
    "#print('\\nTest set score:', svm.score(X_test, y_test))\n",
    "#y_pred2 = svm.predict(X_test)\n",
    "#pd.crosstab(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.63      0.65      5999\n",
      "          1       0.66      0.70      0.68      6001\n",
      "\n",
      "avg / total       0.67      0.67      0.67     12000\n",
      "\n",
      "\n",
      " Testing set score:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.63      0.65      4001\n",
      "          1       0.65      0.69      0.67      3999\n",
      "\n",
      "avg / total       0.66      0.66      0.66      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_train, y_pred1))\n",
    "print('\\n Testing set score:')\n",
    "y_pred2 = svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV from sklearn to find the optimal parameters for C, gamma and kernel from a given set of values to improve our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ............................ C=1, gamma=1, kernel=linear -   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ............................ C=1, gamma=1, kernel=linear -   3.6s\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ............................ C=1, gamma=1, kernel=linear -   4.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] .......................... C=1, gamma=0.1, kernel=linear -   7.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] .......................... C=1, gamma=0.1, kernel=linear -   3.7s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] .......................... C=1, gamma=0.1, kernel=linear -   4.0s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ........................ C=1, gamma=0.001, kernel=linear -   6.9s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ........................ C=1, gamma=0.001, kernel=linear -   3.7s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ........................ C=1, gamma=0.001, kernel=linear -   4.1s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] ....................... C=1, gamma=0.0001, kernel=linear -   7.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] ....................... C=1, gamma=0.0001, kernel=linear -   3.8s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] ....................... C=1, gamma=0.0001, kernel=linear -   4.1s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........................... C=10, gamma=1, kernel=linear -  12.5s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........................... C=10, gamma=1, kernel=linear -  40.7s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........................... C=10, gamma=1, kernel=linear -  39.4s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ......................... C=10, gamma=0.1, kernel=linear -  12.4s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ......................... C=10, gamma=0.1, kernel=linear -  41.4s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ......................... C=10, gamma=0.1, kernel=linear -  38.3s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] ....................... C=10, gamma=0.001, kernel=linear -  12.8s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] ....................... C=10, gamma=0.001, kernel=linear -  41.4s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] ....................... C=10, gamma=0.001, kernel=linear -  38.7s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ...................... C=10, gamma=0.0001, kernel=linear -  12.5s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ...................... C=10, gamma=0.0001, kernel=linear -  41.3s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ...................... C=10, gamma=0.0001, kernel=linear -  38.4s\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] .......................... C=100, gamma=1, kernel=linear - 1.8min\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] .......................... C=100, gamma=1, kernel=linear - 2.1min\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] .......................... C=100, gamma=1, kernel=linear - 4.7min\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ........................ C=100, gamma=0.1, kernel=linear - 1.9min\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ........................ C=100, gamma=0.1, kernel=linear - 2.1min\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ........................ C=100, gamma=0.1, kernel=linear - 4.8min\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ...................... C=100, gamma=0.001, kernel=linear - 1.8min\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ...................... C=100, gamma=0.001, kernel=linear - 2.1min\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ...................... C=100, gamma=0.001, kernel=linear - 4.7min\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] ..................... C=100, gamma=0.0001, kernel=linear - 2.0min\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] ..................... C=100, gamma=0.0001, kernel=linear - 2.1min\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] ..................... C=100, gamma=0.0001, kernel=linear - 4.8min\n",
      "[CV] C=1000, gamma=1, kernel=linear ..................................\n",
      "[CV] ......................... C=1000, gamma=1, kernel=linear - 3.1min\n",
      "[CV] C=1000, gamma=1, kernel=linear ..................................\n",
      "[CV] ......................... C=1000, gamma=1, kernel=linear - 4.0min\n",
      "[CV] C=1000, gamma=1, kernel=linear ..................................\n",
      "[CV] ......................... C=1000, gamma=1, kernel=linear - 2.7min\n",
      "[CV] C=1000, gamma=0.1, kernel=linear ................................\n",
      "[CV] ....................... C=1000, gamma=0.1, kernel=linear - 3.0min\n",
      "[CV] C=1000, gamma=0.1, kernel=linear ................................\n",
      "[CV] ....................... C=1000, gamma=0.1, kernel=linear - 4.1min\n",
      "[CV] C=1000, gamma=0.1, kernel=linear ................................\n",
      "[CV] ....................... C=1000, gamma=0.1, kernel=linear - 2.7min\n",
      "[CV] C=1000, gamma=0.001, kernel=linear ..............................\n",
      "[CV] ..................... C=1000, gamma=0.001, kernel=linear - 3.0min\n",
      "[CV] C=1000, gamma=0.001, kernel=linear ..............................\n",
      "[CV] ..................... C=1000, gamma=0.001, kernel=linear - 3.9min\n",
      "[CV] C=1000, gamma=0.001, kernel=linear ..............................\n",
      "[CV] ..................... C=1000, gamma=0.001, kernel=linear - 2.6min\n",
      "[CV] C=1000, gamma=0.0001, kernel=linear .............................\n",
      "[CV] .................... C=1000, gamma=0.0001, kernel=linear - 3.0min\n",
      "[CV] C=1000, gamma=0.0001, kernel=linear .............................\n",
      "[CV] .................... C=1000, gamma=0.0001, kernel=linear - 3.9min\n",
      "[CV] C=1000, gamma=0.0001, kernel=linear .............................\n",
      "[CV] .................... C=1000, gamma=0.0001, kernel=linear - 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 81.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 10, 100, 1000], 'gamma': [1, 0.1, 0.001, 0.0001], 'kernel': ['linear']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.52      0.51      5999\n",
      "          1       0.51      0.50      0.51      6001\n",
      "\n",
      "avg / total       0.51      0.51      0.51     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This was attempt on subsection of the dataset to reduce load but the parameter found did not improve prediction\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred1 = grid.predict(X_train)\n",
    "print(classification_report(y_train, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1 = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred1))\n",
    "print(pd.crosstab(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Mean using Inverse Document Frequency and Term-frequency weights (tfidf) vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 76013\n",
      "Original sentence:  Prime Suspect continues exploits inscrutable dogged seeker truth justice Detective Superintendent Jane Tennison; first three miniseries PS PS PS notable absence founding writer Lynda La Plante credits Imbued gritty reality first three series second three series pit Tennison forces evil coping middle age loneliness indiscretions host personal professional problems resolutions sometimes less ideal PS conjures two stories PS PS single episodes find Tennison seeking justice behalf brutally wronged waging war institutions willing sacrifice interests victims greater good In words prevail Tennison must overcome evil good forces something makes always gray scenarios PS series yet grayer Tennison wars much matter principle finding murderers Very good stuff gets better series series B+\n",
      "Tf_idf vector: {'tennison': 0.5864784240486661, 'plante': 0.11729568480973322, 'indiscretions': 0.10802577914693971, 'seeker': 0.10412355864436079, 'grayer': 0.11414828435331491, 'superintendent': 0.09549740048745019, 'lynda': 0.09710132330979204, 'imbued': 0.09549740048745019, 'founding': 0.09769284851486669, 'inscrutable': 0.09898145985245836, 'dogged': 0.09710132330979204, 'murderers': 0.08071005511128088, 'gray': 0.0726235403728447, 'prevail': 0.09454544805844839, 'institutions': 0.09247503626337329, 'notable': 0.06687617653206465, 'gritty': 0.06551109974666408, 'jane': 0.061021745325057004, 'behalf': 0.0841619836125288, 'host': 0.06778525711533762, 'conjures': 0.09710132330979204, 'greater': 0.0651948768303342, 'ideal': 0.07102520659340686, 'pit': 0.07675647042399643, 'coping': 0.08587792670955821, 'interests': 0.07556795631937846, 'absence': 0.07118570655614155, 'continues': 0.06211783385913642, 'suspect': 0.06004498839677363, 'prime': 0.06532027134051517, 'seeking': 0.06766740443627248, 'detective': 0.059005782548968606, 'overcome': 0.06839435045144009, 'wars': 0.06221235324829897, 'problems': 0.0493190435659792, 'sacrifice': 0.07238040700598894, 'willing': 0.059531610318577684, 'brutally': 0.07220153842384368, 'forces': 0.12395513692072037, 'writer': 0.05054665895331263, 'credits': 0.05193891461299187, 'evil': 0.0925119134089728, 'wronged': 0.08742766145060915, 'waging': 0.10527626076529496, 'exploits': 0.0788231463299293, 'miniseries': 0.06886360781231864, 'la': 0.055403272752275426, 'personal': 0.05202275066496178, 'loneliness': 0.07456917785819255, 'truth': 0.05215445278629855, 'stories': 0.04814955641832777, 'professional': 0.05989034905554313, 'stuff': 0.04610247952081453, 'age': 0.04844220867248384, 'scenarios': 0.07850291221236412, 'words': 0.04837500605794962, 'middle': 0.04976626732584605, 'finding': 0.0577368590900869, 'single': 0.04879151415657357, 'principle': 0.07829451445912994, 'series': 0.19854058385543394, 'gets': 0.03576440942801188, 'reality': 0.04885433089132551, 'victims': 0.05836962474248595, 'war': 0.044675644604102685, 'matter': 0.045602435359286916, 'ps': 0.46712221300529605, 'justice': 0.11797618739824439, 'good': 0.06544112393300791, 'makes': 0.03247459771044754, 'second': 0.04105605136819174, 'resolutions': 0.0996875468503508, 'better': 0.02967247416325906, 'episodes': 0.05177314551368382}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_of_review, review_df['Rating'], test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "list_of_review_tfidf=vectorizer.fit_transform(list_of_review)\n",
    "print(\"Number of features: %d\" % list_of_review_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(list_of_review_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(10000)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "Rating     0      1\n",
      "row_0              \n",
      "0       8025  12148\n",
      "1       7106   2721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Reduce it to two components.\n",
    "#X_train_pca = PCA(n_components=0.95).fit_transform(X_train_norm)\n",
    "\n",
    "KMean = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "KMean.fit(X_train_tfidf_csr)\n",
    "\n",
    "y_pred = KMean.predict(X_train_tfidf_csr)\n",
    "\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions:\n",
      "Rating     0      1\n",
      "row_0              \n",
      "0       6110  10648\n",
      "1       9021   4221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=2,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_train_tfidf_csr)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(X_train_tfidf_csr)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict_mini, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "Rating      0      1\n",
      "row_0               \n",
      "0       11401   1756\n",
      "1        3730  13113\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "clf.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train_tfidf_csr)\n",
    "\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8171333333333334\n",
      "\n",
      "Test set score: 0.79795\n"
     ]
    }
   ],
   "source": [
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "print('Training set score:', clf.score(X_train_tfidf_csr, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test_tfidf_csr, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
