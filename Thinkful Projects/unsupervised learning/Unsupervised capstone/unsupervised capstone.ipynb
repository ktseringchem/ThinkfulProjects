{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis of Movie Reviews\n",
    "\n",
    "### by Karma Tsering\n",
    "My goal is to predict sentiment of each movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation of this project\n",
    "There are too many movies and too many people with opnion on them. Its doubtful that any one would want to read these reviews, certly not companies that need to \n",
    "\n",
    "### Data Source\n",
    "This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. \n",
    "\n",
    "The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg). \n",
    "\n",
    "In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saves list of files names to loop though\n",
    "neg_train_file_names = [f for f in listdir('aclImdb\\\\train\\\\neg') if isfile(join('aclImdb\\\\train\\\\neg', f))]\n",
    "pos_train_file_names = [f for f in listdir('aclImdb\\\\train\\\\pos') if isfile(join('aclImdb\\\\train\\\\pos', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The reviews are saved in each individual files, so I looped through each file and saved the content of the files as \n",
    "#and assigned a sentement based on which folder the file was in (neg = 0/ pos = 1).\n",
    "review_df1 = []\n",
    "review_df2 = []\n",
    "\n",
    "for file in neg_train_file_names:\n",
    "    file1_open = open(\"aclImdb\\\\train\\\\neg\\\\{}\".format(file), encoding=\"utf8\")\n",
    "    file1_content = file1_open.read()\n",
    "    review_df1.append([file1_content, 0])\n",
    "    \n",
    "for file in pos_train_file_names:\n",
    "    file1_open = open(\"aclImdb\\\\train\\\\pos\\\\{}\".format(file), encoding=\"utf8\")\n",
    "    file1_content = file1_open.read()\n",
    "    review_df2.append([file1_content, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put them in DataFrame format\n",
    "review_df1 = pd.DataFrame(review_df1)\n",
    "review_df2 = pd.DataFrame(review_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was quite possibly the worst movie I have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I guess this would be a great movie for a true...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"They All Laughed\" is one of those little movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not a huge Freddy Krueger fan,but that doe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To me this was Colin Farrells best movie evr! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Several features of this film immediately date...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In 2004, I liked it. Then it became very stupi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I saw this important intense film tonight. Its...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What a poor image of Professional Police Offic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pearl S.Buck was a brilliant author that was a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  This was quite possibly the worst movie I have...       0\n",
       "1  I guess this would be a great movie for a true...       0\n",
       "2  \"They All Laughed\" is one of those little movi...       1\n",
       "3  I'm not a huge Freddy Krueger fan,but that doe...       0\n",
       "4  To me this was Colin Farrells best movie evr! ...       1\n",
       "5  Several features of this film immediately date...       1\n",
       "6  In 2004, I liked it. Then it became very stupi...       0\n",
       "7  I saw this important intense film tonight. Its...       1\n",
       "8  What a poor image of Professional Police Offic...       0\n",
       "9  Pearl S.Buck was a brilliant author that was a...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put all the above dataframe in one dataframe\n",
    "review_df = pd.concat([review_df1, review_df2]).sample(frac=1).reset_index(drop=True)\n",
    "review_df.columns = ['Review', 'Rating']\n",
    "print(review_df.shape)\n",
    "review_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame created above\n",
    "\n",
    "# text file for making BOW\n",
    "Put all the reviews in one string in perperation for extrating key words to be used as feature later on for creating vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick 2000 words and make BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I cleanded the text as much as possible so to reduce load for nlp processing and to increase the number of vocas for feature\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "allwords = \" \"\n",
    "list_of_review = []\n",
    "Review = []\n",
    "pattern = \"[-*:&$%'\\\\?\\\"/<>()\\d]\"\n",
    "pattern2 = r\"\\bbr\\b\"\n",
    "pattern3 = r'\\bA\\b'\n",
    "pattern4 = r'--'\n",
    "#pattern5 = '.+;$'\n",
    "\n",
    "for review in review_df['Review'][0:15000]:\n",
    "    #cleaning up each review text by removing above patters\n",
    "    mov_review = re.sub(pattern, \"\", review)\n",
    "    mov_review = re.sub(pattern2, \"\", mov_review)\n",
    "    mov_review = re.sub(pattern3, \"\", mov_review)\n",
    "    mov_review = re.sub(pattern4, \"\", mov_review)\n",
    "    #mov_review = re.sub(pattern5, \"\", mov_review)\n",
    "    mov_review = mov_review.split()\n",
    "    mov_review = [x for x in mov_review\n",
    "                if not x == ' '\n",
    "                and not x == 'movie'\n",
    "                and not x == '-PRON-'\n",
    "                and not x == 'film'\n",
    "                and not x == '\\x96']\n",
    "\n",
    "    Onefull_review = ''\n",
    "    for review_words in mov_review:\n",
    "        Onefull_review = Onefull_review + ' ' + review_words\n",
    "    list_of_review.append(Onefull_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_of_review = pd.DataFrame(list_of_review)\n",
    "list_of_review.columns = ['Review']\n",
    "list_of_review['Rating'] = review_df['Rating'][0:15000]\n",
    "list_of_review.to_csv(\"list_of_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was quite possibly the worst I have ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I guess this would be a great for a true beli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They All Laughed is one of those little movie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Im not a huge Freddy Krueger fan,but that doe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To me this was Colin Farrells best evr! He in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0   This was quite possibly the worst I have ever...       0\n",
       "1   I guess this would be a great for a true beli...       0\n",
       "2   They All Laughed is one of those little movie...       1\n",
       "3   Im not a huge Freddy Krueger fan,but that doe...       0\n",
       "4   To me this was Colin Farrells best evr! He in...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_review = pd.read_csv('list_of_review.csv', encoding=\"ISO-8859-1\").drop('Unnamed: 0', 1)\n",
    "print(list_of_review.shape)\n",
    "list_of_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2>Vectorization of DF using Inverse Document Frequency and Term-frequency weights (tfidf) and create a vector space model.</h1> </center>\n",
    "<br>\n",
    "<br>\n",
    "Vector space model is the representation of our documents as vectors identifiers, and these vectors are calculated according to following equation.\n",
    "<br>\n",
    "\n",
    "$$\\textrm{Term frequency}= TF(t,d) = \\frac{\\textrm{number of times term(t) appears in document(d)}}{\\textrm{total number ofterms in document(d)}}$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\textrm{Inverse Documment Frequency}= IDF(t,D) = \\frac{\\textrm{total number of documents(D)}}{\\textrm{number of documents with the term(t) in it}}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$TFIDF(t,d,D) = TF(t,d)\\cdot IDF(t,D)$$\n",
    "<br>\n",
    "<br>\n",
    "This vector space model could be used for documents query, document classification, and document clustering. For the purpose of this project, I will apply some of the unsupervised models and assigned them to a group: K-Mean, minibatchkmeans, spectral clustering, MeanShift, and affinity propagation. For this vector space model there are two clusters based on the sentemental assignment but as mentioned in the data souce section these sentemental ranged from 0 to 10. Any sentement less than 4 were assigned 0 and any thing above 7 were assigned 1, so these unsupervied models could come up with more than two groups.\n",
    "<br>\n",
    "<br>\n",
    "KMean works by randomly placing centroids in the vector space. The k parameter determines the number of centroids, that forms the clusters, assigned by you. It does two itteration, first it assignes each document to a centroid, based on their euclidean distance, using the vectors and takes an average to find a new centroids. The model repets these two itterations until no point changes cluster membership. I tried clusters of 2 to 4 and the results are below with the code.\n",
    "<center><h4>KMean model with K=2</h4></center>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Rating/Cluster</th>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">2351</td>\n",
    "    <td class=\"tg-0pky\">3610</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">2180</td>\n",
    "    <td class=\"tg-0pky\">859</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br>\n",
    "<br>\n",
    "Mini batch KMean is a variation of the KMean. It does essentially the same exact computation except on small batch of sample data points to improve on the computational requrement.\n",
    "\n",
    "Spectral clustering groups togather based on similarity, a.k.a affinity, employing algorithem such as KNN and radial basis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 38735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorize only 5,000 rows of the review because my computer can't handle more\n",
    "#split the orighal df into train and test sets for comparison leater\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_of_review['Review'], list_of_review['Rating'], test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             #lowercase=True, #convert everything to lower case\n",
    "                             use_idf=True,#inverse document frequencies\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies. Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer to 5,000 row\n",
    "list_of_review_tfidf=vectorizer.fit_transform(list_of_review['Review'])\n",
    "print(\"Number of features: %d\" % list_of_review_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets of thos vetorized data\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(list_of_review_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For graphing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#singular Value Decomposition (SVD), reduce the feature space from 23051 to 10000.\n",
    "#this takes long time to run\n",
    "svd= TruncatedSVD(8000)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Mean\n",
    "##### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "KMean = KMeans(n_clusters=2, random_state=42, n_jobs=-1)\n",
    "KMean.fit(X_train_lsa)\n",
    "\n",
    "y_KMean2 = KMean.predict(X_train_lsa)\n",
    "\n",
    "labels = KMean.labels_\n",
    "print(\"Adjusted Rand Index\")\n",
    "print(metrics.adjusted_rand_score(y_KMean2, y_train))\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means traning clusters against the data:')\n",
    "print(pd.crosstab(y_KMean2, y_train))\n",
    "print(classification_report(y_KMean2, y_train))\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_train_lsa[:, 0], X_train_lsa[:, 1], c=y_KMean2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 clulster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "KMean = KMeans(n_clusters=3, random_state=42, n_jobs=-1)\n",
    "KMean.fit(X_train_lsa)\n",
    "labels = KMean.labels_\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 clulster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "KMean = KMeans(n_clusters=4, random_state=42)\n",
    "KMean.fit(X_train_lsa)\n",
    "\n",
    "labels = KMean.labels_\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minibatchkmeans\n",
    "##### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=2,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_train_lsa)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini2 = minibatchkmeans.predict(X_train_lsa)\n",
    "\n",
    "labels = minibatchkmeans.labels_\n",
    "\n",
    "print(\"Adjusted Rand Index\")\n",
    "print(metrics.adjusted_rand_score(predict_mini2, y_train))\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(predict_mini2, y_train))\n",
    "print(classification_report(predict_mini2, y_train))\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_train_lsa[:, 0], X_train_lsa[:, 1], c=predict_mini2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=3,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_train_lsa)\n",
    "\n",
    "labels = minibatchkmeans.labels_\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=4,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_train_lsa)\n",
    "\n",
    "labels = minibatchkmeans.labels_\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpectralClustering\n",
    "#### 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Declare and fit the model.\n",
    "sc = SpectralClustering(n_clusters=2)\n",
    "sc.fit(X_train_lsa)\n",
    "\n",
    "labels = sc.labels_\n",
    "\n",
    "#Predicted clusters.\n",
    "pred_spectral2 =sc.fit_predict(X_train_lsa)\n",
    "\n",
    "print(\"Adjusted Rand Index\")\n",
    "print(metrics.adjusted_rand_score(pred_spectral2, y_train))\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(pred_spectral2, y_train))\n",
    "print(classification_report(pred_spectral2, y_train))\n",
    "\n",
    "#Graph results.\n",
    "plt.scatter(X_train_lsa[:, 1], X_train_lsa[:, 2], c=pred_spectral2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Declare and fit the model.\n",
    "sc = SpectralClustering(n_clusters=3)\n",
    "sc.fit(X_train_lsa)\n",
    "\n",
    "labels = sc.labels_\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Declare and fit the model.\n",
    "sc = SpectralClustering(n_clusters=4)\n",
    "sc.fit(X_train_lsa)\n",
    "\n",
    "labels = sc.labels_\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Silhouette coefficient:')\n",
    "print(metrics.silhouette_score(X_train_lsa, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 38735) (9000,)\n",
      "Training set score: 0.9686666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>1257</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0     1     2\n",
       "row_0                  \n",
       "0      5220     0     0\n",
       "1       109  1257    28\n",
       "2       145     0  2241"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First trying some classicication models starting with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train_tfidf, y_KMean)\n",
    "\n",
    "print(X_train_tfidf.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_tfidf, y_KMean))\n",
    "\n",
    "y_pred = lr.predict(X_train_tfidf)\n",
    "pd.crosstab(y_KMean, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "col_0     0     1\n",
      "row_0            \n",
      "0      3082   138\n",
      "1       463  5317\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "clf.fit(X_train_tfidf, predict_mini)\n",
    "\n",
    "y_pred = clf.predict(X_train_tfidf)\n",
    "\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, predict_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9332222222222222\n"
     ]
    }
   ],
   "source": [
    "print('Training set score:', clf.score(X_train_tfidf, predict_mini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_train_lsa[:, 0], X_train_lsa[:, 1], X_train_lsa[:, 2], c=y_KMean, zdir='x')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeanShift\n",
    "##### Too much for my computer handel so won't be using these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "bandwidth = estimate_bandwidth(X_train_tfidf, quantile=0.2, n_samples=1000,  n_jobs=-1)\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X_train_tfidf)\n",
    "\n",
    "y_MShift = ms.predict(X_train_tfidf)\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))\n",
    "\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_MShift,  y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Declare the model and fit it in one statement.\n",
    "# Note that you can provide arguments to the model, but we didn't.\n",
    "af = AffinityPropagation().fit(X_train_tfidf)\n",
    "print('Done')\n",
    "\n",
    "# Pull the number of clusters and cluster assignments for each data point.\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    "\n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
