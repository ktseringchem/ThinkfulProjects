{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal\n",
    "\n",
    "* <h6>What is the problem you are attempting to solve?</h6>\n",
    "    * Movie reviews are subjective attitudes, emotions and opinions of people and a sentiment extracts information from these reviews. This sentiment analysis will neatly categorize people's sentiment as positive, negative or neutral and provide a topic summary. \n",
    "\n",
    "\n",
    "* <h6>How is your solution valuable?</h6>\n",
    "    * Movie rating doesn't capture the nuances of the opinions and no one wants to sit and read thousands of reviews. These insights can be used to business decisions such as direct marketing, recommendation system, future project directions.\n",
    "    \n",
    "\n",
    "* <h6>What is your data source and how will you access it?</h6>\n",
    "\t* IMDb movie reviews, using scrapy web scraper.\n",
    "    \n",
    "\n",
    "* <h6>What techniques from the course do you anticipate using?</h6>\n",
    "\t* Web scraping (scrapy): to get the data\n",
    "\t* Natural language processing (spaCy): to process the data and to create features \n",
    "\t* word2vec (Continuous Bag of Words): converting words to vectors\n",
    "\t* Neural Network: to analyze the data\n",
    "\n",
    "\n",
    "* <h6>What do you expect to be the biggest challenge youâ€™ll face?</h6>\n",
    "\t* Web scraping is new to me and it seems you need some understanding of web development.\n",
    "\t* Feature creation: sentence negation, sarcasm, terseness, language ambiguity, slang, misspellings, and many others make this task very challenging.\n",
    "\t* Neural Network optimization: adjusting hyperparameters\n",
    "    * Evaluating results: first stragey is to compare it to existing ratings.\n",
    "\n",
    "\n",
    "\n",
    "Problem Description: \n",
    "* What is the problem that you will be investigating? Why is it interesting?\n",
    "* Data: What data will you use? If you are collecting new datasets, how do you plan to collect them?\n",
    "* Methodology/Algorithm: What method or algorithm are you proposing? If there are existing implementations, will you use them and how? How do you plan to improve or modify such implementations?\n",
    "* Related Work: What reading will you examine to provide context and background?\n",
    "* Evaluation Plan: How will you evaluate your results? Qualitatively, what kind of results do you expect (e.g. plots or figures)? Quantitatively, what kind of analysis will you use to evaluate and/or compare your results (e.g. what performance metrics or statistical tests)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the problem that you will be investigating? Why is it interesting?\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\"> Movie reviews are subjective attitudes, emotions and opinions of people and a sentiment analysis extracts these information from the reviews. This sentiment analysis will neatly categorize people's sentiment as positive, negative or neutral and provide a topic summary.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karma\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# gensim modules\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "\n",
    "#string maluplation\n",
    "import re\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# classifier\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Beautiful soup is the best way to remove html tags form paragraphs.\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#plot liberiry\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#classifiers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Evaluater\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">This dataset contains 50,000 reviews split evenly into a 25k train and 25k test sets, and each review is in individual files. The overall distribution of labels is balanced (25k pos and 25k neg), and those were divided into two folders named pos and neg. This data was collected by  Christopher Potts in 2011 and I used all of it for training by model and collected my own data from IMBd, using scraper called scrapy, of a new movie for testing purposes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  type                                             review label  \\\n",
      "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
      "1           1  test  This is an example of why the majority of acti...   neg   \n",
      "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
      "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
      "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
      "\n",
      "          file  \n",
      "0      0_2.txt  \n",
      "1  10000_4.txt  \n",
      "2  10001_1.txt  \n",
      "3  10002_3.txt  \n",
      "4  10003_3.txt  \n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"imdb_master.csv\", encoding='ISO-8859-1')\n",
    "print(data_df.head())\n",
    "data = data_df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the review text\n",
    "\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Put all the data in a pandas data frame from the individual files and assign sentiment based on the folder the files are in.  Clean the data using Regular expression operations (re), beautiful soup, NLTK and python functions. Since we need to capture the contextual meaning of words and documents, when cleaning the data I need to be careful not to remove too much so as to lose the contextual meaning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a raw review to a string of words\n",
    "# The input is a single string (a raw movie review), and \n",
    "# the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "def review_to_words( raw_review ):\n",
    "    #Beautiful soup is the best way to remove HTML tags form paragraphs.\n",
    "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text() \n",
    "    \n",
    "    #Remove everything but the char after ^   \n",
    "    letters_only = re.sub(\"[^a-zA-Z],.\", \" \", review_text) \n",
    "    \n",
    "    #Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    #Join the words back into one string separated by space, and return the result.\n",
    "    return(nlp(\" \".join( words )))\n",
    "\n",
    "\n",
    "\n",
    "#In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "#stops = set(stopwords.words(\"english\"))                  \n",
    "# Remove stop words (this may not be helpful for this project)\n",
    "#meaningful_words = [w for w in words if not w in stops] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP processed each review\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">NLP tokenenizes tex, meaning it segments the text into individual words and annotations, and returns iterable processed Doc with all the information of the original text.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Using this to creat list of review list</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the text data, apply review_to_words() function created above to raw review data\n",
    "clean_review = []\n",
    "for review in data:\n",
    "    clean_review.append(review_to_words(review))\n",
    "    \n",
    "#parsing the review so to make each review a list of word to pass into Word2Vec\n",
    "clean_review_vocab = []\n",
    "for review in clean_review:\n",
    "    review = [token.lemma_ for token in review]\n",
    "    clean_review_vocab.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list of words in the model\n",
    "vect_df = pd.DataFrame()\n",
    "vect_df['file'] = data_df['file']\n",
    "vect_df['type'] = data_df['type']\n",
    "vect_df['review'] = clean_review_vocab\n",
    "vect_df['label'] = data_df['label']\n",
    "sent_value = {'label': {'neg': 0, 'unsup': 1, 'pos': 2}}\n",
    "vect_df.replace(sent_value, inplace=True)\n",
    "vect_df.to_csv( \"NLPprossReview.csv\", index=False)\n",
    "vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NLPprossReview.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "* Methodology/Algorithm: What method or algorithm are you proposing? If there are existing implementations, will you use them and how? How do you plan to improve or modify such implementations?\n",
    "\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">The meat of this project is a shallow neural network called word2vec. Word2vec is a two-layer neural network that takes in text and outputs their vectors, so it is a vectorizer. The key difference between word2vec and the other vectorizer (e.g. tfidf, frequency, one-hot-encoder) is the word embading, which says somthing about the relation between the words probabilistically. Other vectorizers lose the ordering of the words and they also ignore semantics of the words because words' vectors are equidistance from each other. Word2Vec expects sentences and each sentnce is a list of words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec(\n",
    "    clean_review_vocab,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "#can save the model for continued training later or load and use later\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examin the build model\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match(\"good bad awful terrible\".split()))\n",
    "print(model.wv.doesnt_match(\"awesome bad awful terrible\".split()))\n",
    "print(model.wv.doesnt_match(\"nice pleasant fine excellent\".split()))\n",
    "# Classic test\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph the result of Word2Vec\n",
    "\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">How good is this vector representation?\n",
    "    <br> show this using graphs\n",
    "    <br> Hey Zack I am having trouble graphing this so I will do it later\n",
    "    <br> moving on to doc2vec</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build class to take the average of each review vector to represent that review\n",
    "\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Now that we have vector representation of the words in our corpus, we need to use it to creat vector representation of the documents to pass it into the classifier neurol network. One way to do that is to take the average of the word vectors in the documents and use the averaged word vector to reprsent the documents.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000 == 0:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_2.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>[once, again, mr, costner, have, drag, out, a,...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_4.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>[this, be, an, example, of, why, the, majority...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>[first, of, all, i, hate, those, moronic, rapp...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_3.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>[not, even, the, beatle, could, write, song, e...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_3.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>[brass, picture, movie, be, not, a, fitting, w...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file  type                                             review  \\\n",
       "0      0_2.txt  test  [once, again, mr, costner, have, drag, out, a,...   \n",
       "1  10000_4.txt  test  [this, be, an, example, of, why, the, majority...   \n",
       "2  10001_1.txt  test  [first, of, all, i, hate, those, moronic, rapp...   \n",
       "3  10002_3.txt  test  [not, even, the, beatle, could, write, song, e...   \n",
       "4  10003_3.txt  test  [brass, picture, movie, be, not, a, fitting, w...   \n",
       "\n",
       "      label  \n",
       "0  Negative  \n",
       "1  Negative  \n",
       "2  Negative  \n",
       "3  Negative  \n",
       "4  Negative  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new dataframe with relavent data and some subsititutions\n",
    "vect_df = pd.DataFrame()\n",
    "vect_df['file'] = data_df['file']\n",
    "vect_df['type'] = data_df['type']\n",
    "vect_df['review'] = clean_review_vocab\n",
    "vect_df['label'] = data_df['label']\n",
    "clean_senta = {\"label\": {\"neg\": 'Negative', \"pos\":'Positive', 'unsup': 'Neutral'}}\n",
    "vect_df.replace(clean_senta, inplace=True)\n",
    "vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 6)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the new dataframe segment out the train and test \n",
    "train_data = vect_df[vect_df.type == 'train']\n",
    "test_data = vect_df[vect_df.type == 'test']\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average the vector for each review document and used the averaged vector for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat training and testing datasets\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(train_data['review'], model, num_features)\n",
    "train_sent = vect_df[vect_df.type == 'train'].drop(['file', 'review', 'type'],1)\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(test_data['review'], model, num_features)\n",
    "test_sent = vect_df[vect_df.type == 'test'].drop(['file', 'review', 'type'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish and fit the model, with a triple, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,3))\n",
    "mlp.fit(trainDataVecs, train_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the classifer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22864\n"
     ]
    }
   ],
   "source": [
    "print(mlp.score(testDataVecs, test_sent))\n",
    "\n",
    "#cross_val_score(mlp, test_arrays, test_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mlp_pred = mlp.predict(testDataVecs)\n",
    "#confusion_matrix(test_sent, y_pred)\n",
    "y_mlp_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(trainDataVecs, train_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22864\n"
     ]
    }
   ],
   "source": [
    "print(classifier.score(testDataVecs, test_sent))\n",
    "\n",
    "#cross_val_score(mlp, test_arrays, test_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_logis_pred = classifier.predict(testDataVecs)\n",
    "#confusion_matrix(test_sent, y_pred)\n",
    "y_logis_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "* How does Doc2Vec work?\n",
    "Doc2Vec does essentially the same thing we did above, vectorizes the word and \"aggregates\" the words to represent the document.\n",
    "* The differece is the way it aggregates the vectors, and how does Doc2Vec does that?\n",
    "Doc2Vec treats all the documents as \"labeled\" word and does \"mathameaticl trick\" to represent that word as a vector. The label here will be the name of the file each review contains in.\n",
    "* format\n",
    "Doc2Vec like word to Word2Vec, expects sentences and each sentnce as a list of words, and each sentence is labled. To acomplish this we will be using LabeledSentence class (see code below)\n",
    "* What is this mathamatical trick?\n",
    "labeled documents goes into a neural network and creates word embedding just like the Word2Vec with word window for context and document embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc.split(),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = LabeledLineSentence(clean_review_vocab, data_df['file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">another method is to use doc2vec module\n",
    "<br> build the model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Doc2Vec(\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=1,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    #sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    vector_size=300,      # Word vector length.\n",
    "    hs=1,           # Use hierarchical softmax.\n",
    "    alpha -= 0.0002, # decrease the learing rate\n",
    "    min_alpha = alpha # fix the learing rate, no dacay\n",
    "    \n",
    ")\n",
    "\n",
    "model2.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Here we are training the model by controling the learning rate and iterating over the documents multiple times for better result[1]\n",
    "<br>\n",
    "<br>[1] https://rare-technologies.com/doc2vec-tutorial/ </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n"
     ]
    }
   ],
   "source": [
    "#iterating multiple times trians gives better result as claimed by rare-technologies\n",
    "for epoch in range(10):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model2.train(tagged_data, total_examples=model2.corpus_count, epochs=model2.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\"> not a great result, can be impoved with feature engenering, delet less of text, add more data, and add parts of speach.\n",
    "<br>\n",
    "<br>\n",
    "traing and test data split with vectorization of the documents<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = []\n",
    "trian_labels = []\n",
    "test_arrays = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in vect_df:\n",
    "    if row['label'] == 'neg' and row['type'] == 'trian':\n",
    "        trian_arrays.append(model2[row['file']])\n",
    "        trian_labels.append(0)\n",
    "    elif row['label'] == 'pos' and row['type'] == 'trian':\n",
    "        trian_arrays.append(model2[row['file']])\n",
    "        trian_labels.append(1)\n",
    "    elif row['label'] == 'neg' and row['type'] == 'test':\n",
    "        test_arrays.append(model2[row['file']])\n",
    "        test_labels.append(0)\n",
    "    elif row['label'] == 'pos' and row['type'] == 'test':\n",
    "        test_arrays.append(model2[row['file']])\n",
    "        test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(1000, 3), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,3))\n",
    "mlp.fit(trian_arrays, trian_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5   , 0.6744, 0.6454, 0.6212, 0.6206])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mlp.score(test_arrays, test_labels))\n",
    "\n",
    "cross_val_score(mlp, test_arrays, test_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(trian_arrays, trian_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(test_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.671"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.762 , 0.6708, 0.66  , 0.6236, 0.6226])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(classifier, test_arrays, test_labels, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
    "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
    "636-659."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
