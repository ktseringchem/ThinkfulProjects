{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sans\",\"Droid Sans\",\"Helvetica Neue\",Arial,sans-serif;font-size:16px;line-height:1.5}@media (min-width: 38em){html{font-size:20px}}body{color:#515151;background-color:#fff;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}a{color:#268bd2;text-decoration:none}a:hover,a:focus{text-decoration:underline}a strong{color:inherit}img{display:block;max-width:100%;margin:0 0 1rem;border-radius:5px}table{margin-bottom:1rem;width:100%;font-size:85%;border:1px solid #e5e5e5;border-collapse:collapse}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}th{text-align:left}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}h1,h2,h3,h4,h5,h6{margin-bottom:.5rem;font-weight:600;line-height:1.25;color:#313131;text-rendering:optimizeLegibility}h1{font-size:2rem}h2{margin-top:1rem;font-size:1.5rem}h3{margin-top:1.5rem;font-size:1.25rem}h4,h5,h6{margin-top:1rem;font-size:1rem}p{margin-top:0;margin-bottom:1rem}strong{color:#303030}ul,ol,dl{margin-top:0;margin-bottom:1rem}dt{font-weight:bold}dd{margin-bottom:.5rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee;border-bottom:1px solid #fff}abbr{font-size:85%;font-weight:bold;color:#555;text-transform:uppercase}abbr[title]{cursor:help;border-bottom:1px dotted #e5e5e5}blockquote{padding:.5rem 1rem;margin:.8rem 0;color:#7a7a7a;border-left:.25rem solid #e5e5e5}blockquote p:last-child{margin-bottom:0}@media (min-width: 30em){blockquote{padding-right:5rem;padding-left:1.25rem}}a[href^=\"#fn:\"],a[href^=\"#fnref:\"]{display:inline-block;margin-left:.1rem;font-weight:bold}.footnotes{margin-top:2rem;font-size:85%}.lead{font-size:1.25rem;font-weight:300}.highlight .hll{background-color:#ffc}.highlight .c{color:#999}.highlight .err{color:#a00;background-color:#faa}.highlight .k{color:#069}.highlight .o{color:#555}.highlight .cm{color:#09f;font-style:italic}.highlight .cp{color:#099}.highlight .c1{color:#999}.highlight .cs{color:#999}.highlight .gd{background-color:#fcc;border:1px solid #c00}.highlight .ge{font-style:italic}.highlight .gr{color:red}.highlight .gh{color:#030}.highlight .gi{background-color:#cfc;border:1px solid #0c0}.highlight .go{color:#aaa}.highlight .gp{color:#009}.highlight .gu{color:#030}.highlight .gt{color:#9c6}.highlight .kc{color:#069}.highlight .kd{color:#069}.highlight .kn{color:#069}.highlight .kp{color:#069}.highlight .kr{color:#069}.highlight .kt{color:#078}.highlight .m{color:#f60}.highlight .s{color:#d44950}.highlight .na{color:#4f9fcf}.highlight .nb{color:#366}.highlight .nc{color:#0a8}.highlight .no{color:#360}.highlight .nd{color:#99f}.highlight .ni{color:#999}.highlight .ne{color:#c00}.highlight .nf{color:#c0f}.highlight .nl{color:#99f}.highlight .nn{color:#0cf}.highlight .nt{color:#2f6f9f}.highlight .nv{color:#033}.highlight .ow{color:#000}.highlight .w{color:#bbb}.highlight .mf{color:#f60}.highlight .mh{color:#f60}.highlight .mi{color:#f60}.highlight .mo{color:#f60}.highlight .sb{color:#c30}.highlight .sc{color:#c30}.highlight .sd{color:#c30;font-style:italic}.highlight .s2{color:#c30}.highlight .se{color:#c30}.highlight .sh{color:#c30}.highlight .si{color:#a00}.highlight .sx{color:#c30}.highlight .sr{color:#3aa}.highlight .s1{color:#c30}.highlight .ss{color:#fc3}.highlight .bp{color:#366}.highlight .vc{color:#033}.highlight .vg{color:#033}.highlight .vi{color:#033}.highlight .il{color:#f60}.css .o,.css .o+.nt,.css .nt+.nt{color:#999}code,pre{font-family:Menlo,Monaco,\"Courier New\",monospace}code{padding:.25em .5em;font-size:85%;color:#bf616a;background-color:#f9f9f9;border-radius:3px}pre{margin-top:0;margin-bottom:1rem}pre code{padding:0;font-size:100%;color:inherit;background-color:transparent}.highlight{padding:1rem;margin-bottom:1rem;font-size:.8rem;line-height:1.4;background-color:#f9f9f9;border-radius:.25rem}.highlight pre{margin-bottom:0;overflow-x:auto}.highlight .lineno{display:inline-block;padding-right:.75rem;padding-left:.25rem;color:#999;-webkit-user-select:none;-moz-user-select:none;user-select:none}.container{max-width:38rem;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto}footer{margin-bottom:2rem}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem}.masthead-title{margin-top:0;margin-bottom:0;color:#515151}.masthead-title a{color:inherit}.masthead-title small{font-size:75%;font-weight:400;opacity:.5}.page,.post{margin-bottom:4em}.page li+li,.post li+li{margin-top:.25rem}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:1rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;margin-bottom:2rem;border-top:1px solid #eee;border-bottom:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}.pagination{overflow:hidden;margin:0 -1.5rem 1rem;color:#ccc;text-align:center}.pagination-item{display:block;padding:1rem;border:solid #eee;border-width:1px 0}.pagination-item:first-child{margin-bottom:-1px}a.pagination-item:hover{background-color:#f5f5f5}@media (min-width: 30em){.pagination{margin:3rem 0}.pagination-item{float:left;width:50%;border-width:1px}.pagination-item:first-child{margin-bottom:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination-item:last-child{margin-left:-1px;border-top-right-radius:4px;border-bottom-right-radius:4px}}.message{margin-bottom:1rem;padding:1rem;color:#717171;background-color:#f9f9f9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "*{\n",
    "    -webkit-box-sizing:border-box;\n",
    "    -moz-box-sizing:border-box;\n",
    "    box-sizing:border-box\n",
    "}\n",
    "html,body{\n",
    "    margin:0;\n",
    "    padding:0\n",
    "}\n",
    "html{\n",
    "    font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",\"Roboto\",\"Oxygen\",\"Ubuntu\",\"Cantarell\",\"Fira Sans\",\"Droid Sans\",\"Helvetica Neue\",Arial,sans-serif;font-size:16px;\n",
    "    line-height:1.5\n",
    "}\n",
    "@media (min-width: 38em){\n",
    "    html{\n",
    "        font-size:20px\n",
    "    }\n",
    "}\n",
    "body{\n",
    "    color:#515151;\n",
    "    background-color:#fff;\n",
    "    -webkit-text-size-adjust:100%;\n",
    "    -ms-text-size-adjust:100%\n",
    "}\n",
    "a{\n",
    "    color:#268bd2;\n",
    "    text-decoration:none\n",
    "}\n",
    "a:hover,a:focus{\n",
    "    text-decoration:underline;\n",
    "}\n",
    "        a strong{\n",
    "            color:inherit\n",
    "        }\n",
    "        img{\n",
    "            display:block;\n",
    "            max-width:100%;\n",
    "            margin:0 0 1rem;\n",
    "            border-radius:5px\n",
    "        }\n",
    "        table{\n",
    "            margin-bottom:1rem;\n",
    "            width:100%;\n",
    "            font-size:85%;\n",
    "            border:1px solid #e5e5e5;border-collapse:collapse\n",
    "        }\n",
    "        td,th{\n",
    "            adding:.25rem .5rem;\n",
    "            border:1px solid #e5e5e5\n",
    "        }\n",
    "        th{\n",
    "            text-align:left\n",
    "        }\n",
    "        tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th\n",
    "                {\n",
    "                    background-color:#f9f9f9\n",
    "                }\n",
    "                h1,h2,h3,h4,h5,h6\n",
    "                {margin-bottom:.5rem;font-weight:600;line-height:1.25;color:#313131;text-rendering:optimizeLegibility}h1{font-size:2rem}h2{margin-top:1rem;font-size:1.5rem}h3{margin-top:1.5rem;font-size:1.25rem}h4,h5,h6{margin-top:1rem;font-size:1rem}p{margin-top:0;margin-bottom:1rem}strong{color:#303030}ul,ol,dl{margin-top:0;margin-bottom:1rem}dt{font-weight:bold}dd{margin-bottom:.5rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee;border-bottom:1px solid #fff}abbr{font-size:85%;font-weight:bold;color:#555;text-transform:uppercase}abbr[title]{cursor:help;border-bottom:1px dotted #e5e5e5}blockquote{padding:.5rem 1rem;margin:.8rem 0;color:#7a7a7a;border-left:.25rem solid #e5e5e5}blockquote p:last-child{margin-bottom:0}@media (min-width: 30em){blockquote{padding-right:5rem;padding-left:1.25rem}}a[href^=\"#fn:\"],a[href^=\"#fnref:\"]{display:inline-block;margin-left:.1rem;font-weight:bold}.footnotes{margin-top:2rem;font-size:85%}.lead{font-size:1.25rem;font-weight:300}.highlight .hll{background-color:#ffc}.highlight .c{color:#999}.highlight .err{color:#a00;background-color:#faa}.highlight .k{color:#069}.highlight .o{color:#555}.highlight .cm{color:#09f;font-style:italic}.highlight .cp{color:#099}.highlight .c1{color:#999}.highlight .cs{color:#999}.highlight .gd{background-color:#fcc;border:1px solid #c00}.highlight .ge{font-style:italic}.highlight .gr{color:red}.highlight .gh{color:#030}.highlight .gi{background-color:#cfc;border:1px solid #0c0}.highlight .go{color:#aaa}.highlight .gp{color:#009}.highlight .gu{color:#030}.highlight .gt{color:#9c6}.highlight .kc{color:#069}.highlight .kd{color:#069}.highlight .kn{color:#069}.highlight .kp{color:#069}.highlight .kr{color:#069}.highlight .kt{color:#078}.highlight .m{color:#f60}.highlight .s{color:#d44950}.highlight .na{color:#4f9fcf}.highlight .nb{color:#366}.highlight .nc{color:#0a8}.highlight .no{color:#360}.highlight .nd{color:#99f}.highlight .ni{color:#999}.highlight .ne{color:#c00}.highlight .nf{color:#c0f}.highlight .nl{color:#99f}.highlight .nn{color:#0cf}.highlight .nt{color:#2f6f9f}.highlight .nv{color:#033}.highlight .ow{color:#000}.highlight .w{color:#bbb}.highlight .mf{color:#f60}.highlight .mh{color:#f60}.highlight .mi{color:#f60}.highlight .mo{color:#f60}.highlight .sb{color:#c30}.highlight .sc{color:#c30}.highlight .sd{color:#c30;font-style:italic}.highlight .s2{color:#c30}.highlight .se{color:#c30}.highlight .sh{color:#c30}.highlight .si{color:#a00}.highlight .sx{color:#c30}.highlight .sr{color:#3aa}.highlight .s1{color:#c30}.highlight .ss{color:#fc3}.highlight .bp{color:#366}.highlight .vc{color:#033}.highlight .vg{color:#033}.highlight .vi{color:#033}.highlight .il{color:#f60}.css .o,.css .o+.nt,.css .nt+.nt{color:#999}code,pre{font-family:Menlo,Monaco,\"Courier New\",monospace}code{padding:.25em .5em;font-size:85%;color:#bf616a;background-color:#f9f9f9;border-radius:3px}pre{margin-top:0;margin-bottom:1rem}pre code{padding:0;font-size:100%;color:inherit;background-color:transparent}.highlight{padding:1rem;margin-bottom:1rem;font-size:.8rem;line-height:1.4;background-color:#f9f9f9;border-radius:.25rem}.highlight pre{margin-bottom:0;overflow-x:auto}.highlight .lineno{display:inline-block;padding-right:.75rem;padding-left:.25rem;color:#999;-webkit-user-select:none;-moz-user-select:none;user-select:none}.container{max-width:38rem;padding-left:1.5rem;padding-right:1.5rem;margin-left:auto;margin-right:auto}footer{margin-bottom:2rem}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem}.masthead-title{margin-top:0;margin-bottom:0;color:#515151}.masthead-title a{color:inherit}.masthead-title small{font-size:75%;font-weight:400;opacity:.5}.page,.post{margin-bottom:4em}.page li+li,.post li+li{margin-top:.25rem}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:1rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;margin-bottom:2rem;border-top:1px solid #eee;border-bottom:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}.pagination{overflow:hidden;margin:0 -1.5rem 1rem;color:#ccc;text-align:center}.pagination-item{display:block;padding:1rem;border:solid #eee;border-width:1px 0}.pagination-item:first-child{margin-bottom:-1px}a.pagination-item:hover{background-color:#f5f5f5}@media (min-width: 30em){.pagination{margin:3rem 0}.pagination-item{float:left;width:50%;border-width:1px}.pagination-item:first-child{margin-bottom:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination-item:last-child{margin-left:-1px;border-top-right-radius:4px;border-bottom-right-radius:4px}}.message{margin-bottom:1rem;padding:1rem;color:#717171;background-color:#f9f9f9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">This dataset contains 50,000 reviews split evenly into a 25k train and 25k test sets, and each review is in individual files. The overall distribution of labels is balanced (25k pos and 25k neg), and those were divided into two folders named pos and neg. This data was collected by  Christopher Potts in 2011 and I used all of it for training by model and collected my own data from IMBd, using scraper called scrapy, of a new movie for testing purposes. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal\n",
    "\n",
    "* <h6>What is the problem you are attempting to solve?</h6>\n",
    "    * Movie reviews are subjective attitudes, emotions and opinions of people and a sentiment extracts information from these reviews. This sentiment analysis will neatly categorize people's sentiment as positive, negative or neutral and provide a topic summary. \n",
    "\n",
    "\n",
    "* <h6>How is your solution valuable?</h6>\n",
    "    * Movie rating doesn't capture the nuances of the opinions and no one wants to sit and read thousands of reviews. These insights can be used to business decisions such as direct marketing, recommendation system, future project directions.\n",
    "    \n",
    "\n",
    "* <h6>What is your data source and how will you access it?</h6>\n",
    "\t* IMDb movie reviews, using scrapy web scraper.\n",
    "    \n",
    "\n",
    "* <h6>What techniques from the course do you anticipate using?</h6>\n",
    "\t* Web scraping (scrapy): to get the data\n",
    "\t* Natural language processing (spaCy): to process the data and to create features \n",
    "\t* word2vec (Continuous Bag of Words): converting words to vectors\n",
    "\t* Neural Network: to analyze the data\n",
    "\n",
    "\n",
    "* <h6>What do you expect to be the biggest challenge you’ll face?</h6>\n",
    "\t* Web scraping is new to me and it seems you need some understanding of web development.\n",
    "\t* Feature creation: sentence negation, sarcasm, terseness, language ambiguity, slang, misspellings, and many others make this task very challenging.\n",
    "\t* Neural Network optimization: adjusting hyperparameters\n",
    "    * Evaluating results: first stragey is to compare it to existing ratings.\n",
    "\n",
    "\n",
    "\n",
    "Problem Description: \n",
    "* What is the problem that you will be investigating? Why is it interesting?\n",
    "* Data: What data will you use? If you are collecting new datasets, how do you plan to collect them?\n",
    "* Methodology/Algorithm: What method or algorithm are you proposing? If there are existing implementations, will you use them and how? How do you plan to improve or modify such implementations?\n",
    "* Related Work: What reading will you examine to provide context and background?\n",
    "* Evaluation Plan: How will you evaluate your results? Qualitatively, what kind of results do you expect (e.g. plots or figures)? Quantitatively, what kind of analysis will you use to evaluate and/or compare your results (e.g. what performance metrics or statistical tests)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the problem that you will be investigating? Why is it interesting?\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\"> Movie reviews are subjective attitudes, emotions and opinions of people and a sentiment extracts information from these reviews. This sentiment analysis will neatly categorize people's sentiment as positive, negative or neutral and provide a topic summary.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "#string maluplation\n",
    "import re\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# classifier\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Beautiful soup is the best way to remove html tags form paragraphs.\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">This dataset contains 50,000 reviews split evenly into a 25k train and 25k test sets, and each review is in individual files. The overall distribution of labels is balanced (25k pos and 25k neg), and those were divided into two folders named pos and neg. This data was collected by  Christopher Potts in 2011 and I used all of it for training by model and collected my own data from IMBd, using scraper called scrapy, of a new movie for testing purposes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "docLabels = []\n",
    "docLabels1 = [f for f in listdir('aclImdb\\\\train\\\\neg') if f.endswith('.txt')]\n",
    "docLabels2 = [f for f in listdir('aclImdb\\\\train\\\\pos') if f.endswith('.txt')]\n",
    "docLabels3 = [f for f in listdir('aclImdb\\\\test\\\\neg') if f.endswith('.txt')]\n",
    "docLabels4 = [f for f in listdir('aclImdb\\\\test\\\\pos') if f.endswith('.txt')]\n",
    "docLabels = docLabels1 + docLabels2 + docLabels3 + docLabels4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in docLabels:\n",
    "    if isfile(join('aclImdb\\\\train\\\\neg\\\\', doc)):\n",
    "        open_file = open('aclImdb\\\\train\\\\neg\\\\' + doc, 'r', encoding=\"utf8\")\n",
    "        open_file = open_file.read()\n",
    "        data.append(open_file)\n",
    "    elif isfile(join('aclImdb\\\\train\\\\pos\\\\', doc)):\n",
    "        open_file = open('aclImdb\\\\train\\\\pos\\\\' + doc, 'r', encoding=\"utf8\")\n",
    "        open_file = open_file.read()\n",
    "        data.append(open_file)\n",
    "    elif isfile(join('aclImdb\\\\test\\\\neg\\\\', doc)):\n",
    "        open_file = open('aclImdb\\\\test\\\\neg\\\\' + doc, 'r', encoding=\"utf8\")\n",
    "        open_file = open_file.read()\n",
    "        data.append(open_file)\n",
    "    else:\n",
    "        open_file = open('aclImdb\\\\test\\\\pos\\\\' + doc, 'r', encoding=\"utf8\")\n",
    "        open_file = open_file.read()\n",
    "        data.append(open_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Put all the data in a pandas data frame from the individual files and assign sentiment based on the folder the files are in.  Clean the data using Regular expression operations (re), beautiful soup, NLTK and python functions. Since we need to capture the contextual meaning of words and documents, when cleaning the data I need to be careful not to remove too much so as to lose the contextual meaning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert a raw review to a string of words\n",
    "# The input is a single string (a raw movie review), and \n",
    "# the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "def review_to_words( raw_review ):\n",
    "    #Beautiful soup is the best way to remove HTML tags form paragraphs.\n",
    "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text() \n",
    "    #\n",
    "    #Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    #Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    #In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    #stops = set(stopwords.words(\"english\"))                  \n",
    "    # Remove stop words (this may not be helpful for this project)\n",
    "    #meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    #Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply review_to_words() function created above to raw review data\n",
    "clean_review = []\n",
    "for x in data:\n",
    "    clean_review.append(review_to_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc.split(),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_data = LabeledLineSentence(clean_review, docLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Methodology/Algorithm: What method or algorithm are you proposing? If there are existing implementations, will you use them and how? How do you plan to improve or modify such implementations?\n",
    "\n",
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">The meat of this project is a shallow neural network called word2vec. Word2vec is a two-layer neural network that takes in text and outputs their vectors, so it is a vectorizer. The key difference between word2vec and the other vectorizer (e.g. tfidf, frequency, one-hot-encoder) is the word embading, which says somthing about the relation between the words probabilistically. Other vectorizers lose the ordering of the words and they also ignore semantics of the words because words' vectors are equidistance from each other.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(\n",
    "    list_of_reviews,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "#can save the model for continued training later or load and use later\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>vocab_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674226</td>\n",
       "      <td>0.413999</td>\n",
       "      <td>-0.191157</td>\n",
       "      <td>0.412055</td>\n",
       "      <td>0.198682</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>-0.745847</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>-1.134042</td>\n",
       "      <td>0.178597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023496</td>\n",
       "      <td>-0.020488</td>\n",
       "      <td>-0.286641</td>\n",
       "      <td>-0.515498</td>\n",
       "      <td>0.454745</td>\n",
       "      <td>0.885410</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>-0.049568</td>\n",
       "      <td>-0.011023</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.328533</td>\n",
       "      <td>0.435401</td>\n",
       "      <td>0.239809</td>\n",
       "      <td>-0.855035</td>\n",
       "      <td>-0.299644</td>\n",
       "      <td>-0.536809</td>\n",
       "      <td>0.291407</td>\n",
       "      <td>-0.284222</td>\n",
       "      <td>-0.722225</td>\n",
       "      <td>1.159497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344035</td>\n",
       "      <td>-0.564331</td>\n",
       "      <td>0.495891</td>\n",
       "      <td>0.631248</td>\n",
       "      <td>-0.594410</td>\n",
       "      <td>-0.922665</td>\n",
       "      <td>-0.512988</td>\n",
       "      <td>0.841829</td>\n",
       "      <td>0.158342</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068361</td>\n",
       "      <td>0.402387</td>\n",
       "      <td>-0.402625</td>\n",
       "      <td>1.044695</td>\n",
       "      <td>0.423520</td>\n",
       "      <td>-0.200199</td>\n",
       "      <td>-0.787988</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>-0.227085</td>\n",
       "      <td>0.336626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020124</td>\n",
       "      <td>0.580071</td>\n",
       "      <td>-0.067423</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>0.442019</td>\n",
       "      <td>1.007197</td>\n",
       "      <td>0.748192</td>\n",
       "      <td>-0.028187</td>\n",
       "      <td>0.417967</td>\n",
       "      <td>possibly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.814146</td>\n",
       "      <td>-1.120861</td>\n",
       "      <td>-0.676685</td>\n",
       "      <td>1.929251</td>\n",
       "      <td>-0.184621</td>\n",
       "      <td>0.664005</td>\n",
       "      <td>-0.402437</td>\n",
       "      <td>-0.099938</td>\n",
       "      <td>-0.725896</td>\n",
       "      <td>0.273354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090680</td>\n",
       "      <td>0.344576</td>\n",
       "      <td>0.535061</td>\n",
       "      <td>-0.872072</td>\n",
       "      <td>0.987788</td>\n",
       "      <td>-0.049637</td>\n",
       "      <td>0.225583</td>\n",
       "      <td>0.685698</td>\n",
       "      <td>0.627497</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.393544</td>\n",
       "      <td>-0.488274</td>\n",
       "      <td>-0.941881</td>\n",
       "      <td>0.055469</td>\n",
       "      <td>0.310603</td>\n",
       "      <td>0.294286</td>\n",
       "      <td>-0.236678</td>\n",
       "      <td>0.359380</td>\n",
       "      <td>-0.201362</td>\n",
       "      <td>0.236543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318816</td>\n",
       "      <td>-0.167705</td>\n",
       "      <td>-0.071356</td>\n",
       "      <td>0.031614</td>\n",
       "      <td>0.319891</td>\n",
       "      <td>0.108595</td>\n",
       "      <td>-0.398131</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>0.084882</td>\n",
       "      <td>-pron-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.674226  0.413999 -0.191157  0.412055  0.198682  0.122666 -0.745847   \n",
       "1  0.328533  0.435401  0.239809 -0.855035 -0.299644 -0.536809  0.291407   \n",
       "2  0.068361  0.402387 -0.402625  1.044695  0.423520 -0.200199 -0.787988   \n",
       "3 -0.814146 -1.120861 -0.676685  1.929251 -0.184621  0.664005 -0.402437   \n",
       "4 -0.393544 -0.488274 -0.941881  0.055469  0.310603  0.294286 -0.236678   \n",
       "\n",
       "          7         8         9    ...           291       292       293  \\\n",
       "0  0.122400 -1.134042  0.178597    ...      0.023496 -0.020488 -0.286641   \n",
       "1 -0.284222 -0.722225  1.159497    ...     -0.344035 -0.564331  0.495891   \n",
       "2  0.129094 -0.227085  0.336626    ...     -0.020124  0.580071 -0.067423   \n",
       "3 -0.099938 -0.725896  0.273354    ...      0.090680  0.344576  0.535061   \n",
       "4  0.359380 -0.201362  0.236543    ...      0.318816 -0.167705 -0.071356   \n",
       "\n",
       "        294       295       296       297       298       299  vocab_key  \n",
       "0 -0.515498  0.454745  0.885410  0.347443 -0.049568 -0.011023             \n",
       "1  0.631248 -0.594410 -0.922665 -0.512988  0.841829  0.158342       this  \n",
       "2 -0.571375  0.442019  1.007197  0.748192 -0.028187  0.417967   possibly  \n",
       "3 -0.872072  0.987788 -0.049637  0.225583  0.685698  0.627497        bad  \n",
       "4  0.031614  0.319891  0.108595 -0.398131  0.372208  0.084882     -pron-  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examin the build model\n",
    "vocab = model.wv.vocab.keys()\n",
    "vect_df = pd.DataFrame(model.wv.vectors)\n",
    "vect_df['vocab_key'] = vocab\n",
    "vect_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">How good is this vector representation?\n",
    "    <br> show this using graphs\n",
    "    <br> Hey Zack I am having trouble graphing this so I will do it later\n",
    "    <br> moving on to doc2vec</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = sklearn.manifold.TSNE(n_components = 0 , random_state = 0)\n",
    "all_vector_matrix = vect_df.drop('vocab_key',1)\n",
    "all_vector_matrix_2d = tsne.fit_transform(all_vector_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Now that we have vector representation of the words in our corpus, we need to use it to creat vector representation of the documents to pass it into the classifier neurol network. There are couple of previous methods for acomplish this, one of which I will implement here term-frequency-inverse-document-frequency (tfidf)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">another method is to use doc2vec module\n",
    "<br> build the model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=1,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    #sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    vector_size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">Here we are training the model by controling the learning rate and iterating over the documents multiple times for better result[1]\n",
    "<br>\n",
    "<br>[1] https://rare-technologies.com/doc2vec-tutorial/ </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\">doc2vec is build on top of word2vec so we get word vectors and can be tested.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktser\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\ktser\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('entirety', 0.25249195098876953),\n",
       " ('subsequent', 0.23595638573169708),\n",
       " ('dwivedi', 0.22563199698925018),\n",
       " ('esp', 0.2254733145236969),\n",
       " ('cheta', 0.22267746925354004),\n",
       " ('riefenstahl', 0.22252562642097473),\n",
       " ('anchorwoman', 0.22070996463298798),\n",
       " ('avantegardistic', 0.21919968724250793),\n",
       " ('davinci', 0.21898135542869568),\n",
       " ('halla', 0.2186593860387802)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('future')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Helvetica Neue;font-size:20px\"> not a great result, can be impoved with feature engenering, delet less of text, add more data, and add parts of speach.\n",
    "<br>\n",
    "<br>\n",
    "traing and test data split with vectorization of the documents<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trian_arrays = numpy.zeros((25000, 300))\n",
    "trian_labels = numpy.zeros(25000)\n",
    "\n",
    "for i, file_name in enumerate(docLabels1 + docLabels2):\n",
    "    if i < (12500):\n",
    "        trian_arrays[i] = model[file_name]\n",
    "        trian_labels[i] = 0\n",
    "    elif i >= (12500):\n",
    "        trian_arrays[i] = model[file_name]\n",
    "        trian_labels[i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 300))\n",
    "test_labels = numpy.zeros(25000)\n",
    "\n",
    "for i, file_name in enumerate(docLabels3 + docLabels4):\n",
    "    if i < (12500):\n",
    "        test_arrays[i] = model[file_name]\n",
    "        test_labels[i] = 0\n",
    "    elif i >= (12500):\n",
    "        test_arrays[i] = model[file_name]\n",
    "        test_labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(1000, 3), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alright! We've done our prep, let's build the model.\n",
    "# Neural networks are hugely computationally intensive.\n",
    "# This may take several minutes to run.\n",
    "\n",
    "# Import the model.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,3))\n",
    "mlp.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75444911, 0.7094    , 0.685     , 0.657     , 0.65193039])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(mlp, test_arrays, test_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, trian_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(test_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12500,     0],\n",
       "       [12500,     0]], dtype=int64)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.793 , 0.7178, 0.7014, 0.6696, 0.6606])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(classifier, test_arrays, test_labels, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
    "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
    "636-659."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
